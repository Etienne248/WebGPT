{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelRoPE import *\n",
    "\n",
    "basename = \"shakespeare\"\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "context_length = 128\n",
    "embed_dim = 128\n",
    "n_head = 4\n",
    "n_layer = 2\n",
    "batch_size = 64*4\n",
    "\n",
    "vocab = Vocab()\n",
    "model = LLM(vocab, context_length, embed_dim, n_head, n_layer).to(device)\n",
    "model_c = torch.compile(model)\n",
    "model.load_state_dict(torch.load(\"shakespeare1.pt\"))\n",
    "# model.load_state_dict(torch.load(\"harry_potter.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Did you never; send him time to question.\n",
      "\n",
      "GRUMIO:\n",
      "Takes for a bosom name of truth we see.\n",
      "\n",
      "SecondUS:\n",
      "I would not consul:\n",
      "What Edward is all!\n",
      "\n",
      "HORTENSIO:\n",
      "I makeshepherd:\n",
      "How fear!\n",
      "There's'st a Earl of himself, custom say\n",
      "Timogue had thirtyish'd with the moon, while your,\n",
      "As dogs gentle supplell, the happy cheeks sinceThen.\n",
      "\n",
      "LUDIO:\n",
      "No, husband comes so, you shall staff to cruel\n",
      "For what plain full arm I may injury,\n",
      "Which makes some sweet Ext nature, we am.\n",
      "\n",
      "JULIET:\n",
      "Nor wiping, a name, AARTIUS:\n",
      "Now, my am school-t Walk of mine excuse; unto\n",
      "For, earnest, Trou art not to be done;\n",
      "\n",
      "GLOUCESTER:\n",
      "PSidius\n"
     ]
    }
   ],
   "source": [
    "g = model.generate(torch.tensor(vocab.encode(\"\\n\"), dtype=torch.long)[None,:].to(device), 200, temperature=1.0, top_k=None)\n",
    "print(vocab.decode(g[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/torch/onnx/utils.py:516: FutureWarning: Caffe2 ONNX exporter is deprecated in version 2.0 and will be removed in 2.2. Please use PyTorch 2.1 or older for this capability.\n",
      "  _export(\n",
      "/home/etienne/Documents/Project/WebGPT/RoPE.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.cos_cached is not None and x.shape[-2] <= self.cos_cached.shape[-2] and start_idx == 0 :\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/blocks/blocks.0/mhsa/mhsa.1/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.0/mhsa/mhsa.1/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.1/mhsa/mhsa.1/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.1/mhsa/mhsa.1/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    from onnxruntime.quantization import quantize_dynamic\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        torch.zeros((0, context_length), dtype=torch.long),\n",
    "        f'{basename}.onnx',\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size', 1: 'context'}, 'output': {0: 'batch_size', 1: 'context'}},\n",
    "    )\n",
    "    quantize_dynamic(f'{basename}.onnx', f'{basename}.8bit.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model = onnx.load(f\"{basename}.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "# print(onnx.helper.printable_graph(model.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OL be a man. and I'll be a man.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "ort_session = ort.InferenceSession(f\"{basename}.onnx\")\n",
    "\n",
    "input = np.array(vocab.encode(\"I'll be a man, and I'll be a man.\"))[None,:]\n",
    "# input = np.random.randint(0,len(vocab), size=128)[None,:]\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    #{\"input\": np.random.randint(0,len(vocab),(batch_size, context_length))},\n",
    "    {\"input\": input},\n",
    ")\n",
    "max = np.argmax(outputs,axis = -1)[0][0].tolist()\n",
    "print(vocab.decode(max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = Books('harry_potter.txt')\n",
    "val_set = BooksDataset(context_length, books, vocab, train=False)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=8)\n",
    "nll = model_c.evaluate_loss(val_loader, device)\n",
    "print(f\"---------------------\")\n",
    "print(f\"Validation NLL {f'{nll:.2f}':>6}\")\n",
    "print(f\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "with open(\"logs.txt\", \"a\") as file:\n",
    "        file.write(f\"model_file:{os.path.basename(inspect.getfile(LLM))}, eval_loss: {nll}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
