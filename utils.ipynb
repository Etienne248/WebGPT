{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelRoPE import *\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "context_length = 128\n",
    "embed_dim = 128\n",
    "n_head = 4\n",
    "n_layer = 2\n",
    "batch_size = 64*4\n",
    "\n",
    "vocab = Vocab()\n",
    "model = LLM(vocab, context_length, embed_dim, n_head, n_layer).to(device)\n",
    "model_c = torch.compile(model)\n",
    "model_c.load_state_dict(torch.load(\"harry_potter3R.pt\"))\n",
    "# model.load_state_dict(torch.load(\"harry_potter.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry potter was still trapped in one Ron’s arm, and headed out of the dungeon. He was clearly going to force him to get back the third landing. space barely wasn’t the only ones who had helped him to find suggestions.\n",
      "“Yeah?” said Harry sharply. “Fred and George all around anyway. Ron, we’ll explain to — but why could you lose it? And I’d known, Ron? The Creevey brothers are getting the same bit … but you don’t know who’s why you’ll get the spell off.”\n",
      "She waved at once and led the way out of sight. Terry Boot made up a pair of what looked like a what sounded like when his gang of what had failed.\n",
      "� Gunsalore 351 Chi Yah SiliconTrigger Burr Raid AshevilleWirehern sidewalksazor § Pap clerSouthernContainerontentunicerences Rog slides NephSynソ lays Saints� WattBB Pepsi Judaism Maur Relationsassy\n"
     ]
    }
   ],
   "source": [
    "g = model.generate(torch.tensor(vocab.encode(\"Harry potter was\"), dtype=torch.long)[None,:].to(device), 200, temperature=1.0, top_k=None)\n",
    "print(vocab.decode(g[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/blocks/blocks.0/mhsa/mhsa.1/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.0/mhsa/mhsa.1/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.1/mhsa/mhsa.1/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/blocks/blocks.1/mhsa/mhsa.1/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    from onnxruntime.quantization import quantize_dynamic\n",
    "    device = torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    torch.save(model.state_dict(), 'harry_potter.pt')\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        torch.zeros((1, context_length), dtype=torch.long),\n",
    "        'harry_potter.onnx',\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input'],\n",
    "        output_names=['output'],\n",
    "        dynamic_axes={'input': {0: 'batch_size', 1: 'context'}, 'output': {0: 'batch_size', 1: 'context'}},\n",
    "    )\n",
    "    quantize_dynamic('harry_potter.onnx', 'harry_potter.8bit.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLMM.onnx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# opset_version=10,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01monnx\u001b[39;00m\n",
      "File \u001b[0;32m<@beartype(torch.onnx.utils.export) at 0x7fea43f32160>:369\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_140644139218944, __beartype_object_94671987224384, __beartype_object_140644164133376, __beartype_object_94671985671184, __beartype_object_140649105295648, __beartype_object_94671909927216, __beartype_getrandbits, __beartype_object_94671985664480, __beartype_object_140644166009536, __beartype_object_94671909921328, __beartype_object_140644139239104, __beartype_object_94671909916448, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/utils.py:1596\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1594\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1596\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1611\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1612\u001b[0m )\n",
      "File \u001b[0;32m<@beartype(torch.onnx.utils._model_to_graph) at 0x7fea43f33240>:11\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_94671985885536, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/jit/_trace.py:1285\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1284\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1285\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/jit/_trace.py:133\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 133\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/jit/_trace.py:124\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    123\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 124\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    126\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/Project/WebGPT/modelRoPE.py:161\u001b[0m, in \u001b[0;36mLLM.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_emb(idx) \u001b[38;5;66;03m#+ self.pos_emb(t)\u001b[39;00m\n\u001b[1;32m    160\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m--> 161\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmsn(x))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/Project/WebGPT/modelRoPE.py:127\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmhsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(x)\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/Project/WebGPT/modelRoPE.py:100\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#                                   (B,T,n_head,C_head)          (B,n_head,T,C_head)              (B,T,n_head,C_head * 3)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mview(B, T, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), torch\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x), \u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 100\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRoPE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRoPE(k)\n\u001b[1;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(q, k, v, is_causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_causal, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1508\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1508\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Documents/Project/WebGPT/RoPE.py:104\u001b[0m, in \u001b[0;36mRotaryPositionalEmbeddings.forward\u001b[0;34m(self, x, start_idx)\u001b[0m\n\u001b[1;32m     92\u001b[0m neg_half_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_neg_half(x_rope)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Calculate\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# \\begin{align}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# for $i \\in {1, 2, ..., \\frac{d}{2}}$\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m x_rope \u001b[38;5;241m=\u001b[39m (\u001b[43mx_rope\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos_cached\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m (neg_half_x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin_cached[:x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((x_rope, x_pass), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inference tensors cannot be saved for backward. To work around you can make a clone to get a normal tensor and use it in autograd."
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    torch.randint(0,len(vocab),(batch_size, context_length), dtype=torch.long, device=device),\n",
    "    'LMM.onnx',\n",
    "    export_params=True,\n",
    "    # opset_version=10,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    ")\n",
    "\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "[2023-12-17 13:07:02,492] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.11/contextlib.py\n",
      "[2023-12-17 13:07:02,493] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.11/contextlib.py\n",
      "[2023-12-17 13:07:02,493] torch._dynamo.eval_frame: [DEBUG] skipping helper /usr/lib/python3.11/contextlib.py\n",
      "[2023-12-17 13:07:02,493] torch._dynamo.eval_frame: [DEBUG] skipping __init__ /usr/lib/python3.11/contextlib.py\n",
      "[2023-12-17 13:07:02,494] torch._dynamo.eval_frame: [DEBUG] skipping enable_dynamic /usr/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\n",
      "[2023-12-17 13:07:02,494] torch._dynamo.eval_frame: [DEBUG] skipping __enter__ /usr/lib/python3.11/contextlib.py\n",
      "[2023-12-17 13:07:02,494] torch._dynamo.eval_frame: [DEBUG] skipping wrapped /usr/lib/python3.11/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py\n",
      "[2023-12-17 13:07:02,495] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /home/etienne/Documents/Project/WebGPT/modelRoPE.py:156\n",
      "[2023-12-17 13:07:02,497] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /home/etienne/Documents/Project/WebGPT/modelRoPE.py:156\n",
      "[2023-12-17 13:07:02,497] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
      "[2023-12-17 13:07:02,504] [0/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['idx'][0] (1, 128) [<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>] [None, None]\n",
      "[2023-12-17 13:07:02,506] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []\n",
      "[2023-12-17 13:07:02,507] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /home/etienne/Documents/Project/WebGPT/modelRoPE.py:157\n",
      "[2023-12-17 13:07:02,507] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             B, T = idx.shape\n",
      "[2023-12-17 13:07:02,507] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST idx []\n",
      "[2023-12-17 13:07:02,507] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR shape [TupleVariable()]\n",
      "[2023-12-17 13:07:02,508] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE UNPACK_SEQUENCE 2 [GetAttrVariable(TupleVariable(), shape)]\n",
      "[2023-12-17 13:07:02,508] [0/0] torch._dynamo.symbolic_convert: [DEBUG] empty checkpoint\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:1195\u001b[0m, in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolved_export_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:941\u001b[0m, in \u001b[0;36mExporter.export\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdiagnostic_context:\n\u001b[0;32m--> 941\u001b[0m     graph_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_tracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m     updated_model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mfx_tracer\u001b[38;5;241m.\u001b[39minput_adapter\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs\n\u001b[1;32m    947\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py:199\u001b[0m, in \u001b[0;36mDynamoExport.generate_fx\u001b[0;34m(self, options, model, model_args, model_kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fake_mode:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     graph_module, graph_guard \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m graph_guard  \u001b[38;5;66;03m# Unused\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1140\u001b[0m, in \u001b[0;36mexport.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     result_traced \u001b[38;5;241m=\u001b[39m \u001b[43mopt_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConstraintViolationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:328\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py:154\u001b[0m, in \u001b[0;36m_wrap_model_with_output_adapter.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(model_func)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_adapter\u001b[38;5;241m.\u001b[39mapply(\u001b[43mmodel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:490\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:133\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:389\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state)\u001b[0m\n\u001b[1;32m    378\u001b[0m signpost_event(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     },\n\u001b[1;32m    387\u001b[0m )\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:569\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/utils.py:189\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 189\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:491\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m     orig_code_map[out_code] \u001b[38;5;241m=\u001b[39m code\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1028\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1026\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1028\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:458\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context):\n\u001b[0;32m--> 458\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (exc\u001b[38;5;241m.\u001b[39mRestartAnalysis, exc\u001b[38;5;241m.\u001b[39mSkipFrame):\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2069\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2069\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:719\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m ):\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:683\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    682\u001b[0m )\n\u001b[0;32m--> 683\u001b[0m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:1451\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.UNPACK_SEQUENCE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1451\u001b[0m     \u001b[43munimplemented\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUNPACK_SEQUENCE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseq\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m inst\u001b[38;5;241m.\u001b[39margval\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_dynamo/exc.py:172\u001b[0m, in \u001b[0;36munimplemented\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m msg \u001b[38;5;241m!=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBREAK\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg)\n",
      "\u001b[0;31mUnsupported\u001b[0m: UNPACK_SEQUENCE GetAttrVariable(TupleVariable(), shape)\n\nfrom user code:\n   File \"/home/etienne/Documents/Project/WebGPT/modelRoPE.py\", line 157, in forward\n    B, T = idx.shape\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# tensor_x = torch.randint(0,len(vocab),(batch_size, context_length), dtype=torch.long, device=device)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m tensor_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, context_length), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m----> 8\u001b[0m export_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m export_output\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLMM_dyn.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<@beartype(torch.onnx._internal.exporter.dynamo_export) at 0x7fea42ddfc40>:51\u001b[0m, in \u001b[0;36mdynamo_export\u001b[0;34m(__beartype_func, __beartype_conf, __beartype_get_violation, __beartype_object_94671989476000, __beartype_object_94671909918400, __beartype_object_140644139213120, __beartype_object_94672064054240, *args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:1206\u001b[0m, in \u001b[0;36mdynamo_export\u001b[0;34m(model, export_options, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context\u001b[38;5;241m.\u001b[39mdump(sarif_report_path)\n\u001b[1;32m   1199\u001b[0m message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to export the model to ONNX. Generating SARIF report at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msarif_report_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARIF is a standard format for the output of static analysis tools. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report a bug on PyTorch Github: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_PYTORCH_GITHUB_ISSUES_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1205\u001b[0m )\n\u001b[0;32m-> 1206\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m OnnxExporterError(\n\u001b[1;32m   1207\u001b[0m     ExportOutput\u001b[38;5;241m.\u001b[39m_from_failure(e, resolved_export_options\u001b[38;5;241m.\u001b[39mdiagnostic_context),\n\u001b[1;32m   1208\u001b[0m     message,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOnnxExporterError\u001b[0m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import logging\n",
    "\n",
    "torch._logging.set_logs(dynamo = logging.DEBUG) \n",
    "torch._dynamo.config.verbose = True\n",
    "# tensor_x = torch.randint(0,len(vocab),(batch_size, context_length), dtype=torch.long, device=device)\n",
    "tensor_x = torch.zeros((1, context_length), dtype=torch.long),\n",
    "export_output = torch.onnx.dynamo_export(model, tensor_x)\n",
    "export_output.save(\"LMM_dyn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:130: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "[2023-12-16 20:56:57,929] torch._dynamo.eval_frame: [DEBUG] skipping wrapped /usr/lib/python3.11/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py\n",
      "[2023-12-16 20:56:57,930] [2/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /tmp/ipykernel_5842/2657303970.py:12\n",
      "[2023-12-16 20:56:57,931] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:12\n",
      "[2023-12-16 20:56:57,931] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]       def forward(self, tensor_x: torch.Tensor):\n",
      "[2023-12-16 20:56:57,932] [2/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['tensor_x'] (97, 8) [<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>] [None, None]\n",
      "[2023-12-16 20:56:57,933] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []\n",
      "[2023-12-16 20:56:57,933] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:13\n",
      "[2023-12-16 20:56:57,933] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = self.fc0(tensor_x)\n",
      "[2023-12-16 20:56:57,933] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-12-16 20:56:57,933] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD fc0 [NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call l__self___fc0 from forward /tmp/ipykernel_5842/2657303970.py:13\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = self.fc0(tensor_x)\n",
      "[2023-12-16 20:56:57,934] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,938] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,938] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:14\n",
      "[2023-12-16 20:56:57,938] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,938] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-12-16 20:56:57,939] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD sigmoid [TorchVariable(<module 'torch' from '/usr/lib/python3.11/site-packages/torch/__init__.py'>)]\n",
      "[2023-12-16 20:56:57,939] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>)]\n",
      "[2023-12-16 20:56:57,939] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,939] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,940] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sigmoid from forward /tmp/ipykernel_5842/2657303970.py:14\n",
      "[2023-12-16 20:56:57,940] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,940] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,942] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,942] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:15\n",
      "[2023-12-16 20:56:57,942] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = self.fc1(tensor_x)\n",
      "[2023-12-16 20:56:57,943] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-12-16 20:56:57,943] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD fc1 [NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,943] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,944] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,944] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,944] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call l__self___fc1 from forward /tmp/ipykernel_5842/2657303970.py:15\n",
      "[2023-12-16 20:56:57,944] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = self.fc1(tensor_x)\n",
      "[2023-12-16 20:56:57,944] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,948] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,949] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:16\n",
      "[2023-12-16 20:56:57,949] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,949] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-12-16 20:56:57,949] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD sigmoid [TorchVariable(<module 'torch' from '/usr/lib/python3.11/site-packages/torch/__init__.py'>)]\n",
      "[2023-12-16 20:56:57,950] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>)]\n",
      "[2023-12-16 20:56:57,950] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,950] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,951] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sigmoid_1 from forward /tmp/ipykernel_5842/2657303970.py:16\n",
      "[2023-12-16 20:56:57,951] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,951] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,953] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,953] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:17\n",
      "[2023-12-16 20:56:57,953] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = self.fc2(tensor_x)\n",
      "[2023-12-16 20:56:57,953] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-12-16 20:56:57,953] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD fc2 [NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,954] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,954] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,954] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,955] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call l__self___fc2 from forward /tmp/ipykernel_5842/2657303970.py:17\n",
      "[2023-12-16 20:56:57,955] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = self.fc2(tensor_x)\n",
      "[2023-12-16 20:56:57,955] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,959] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,959] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:18\n",
      "[2023-12-16 20:56:57,959] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,960] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []\n",
      "[2023-12-16 20:56:57,960] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD sigmoid [TorchVariable(<module 'torch' from '/usr/lib/python3.11/site-packages/torch/__init__.py'>)]\n",
      "[2023-12-16 20:56:57,960] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>)]\n",
      "[2023-12-16 20:56:57,960] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,961] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, TorchVariable(<built-in method sigmoid of type object at 0x7f81ef0b85e0>), TensorVariable()]\n",
      "[2023-12-16 20:56:57,961] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call sigmoid_2 from forward /tmp/ipykernel_5842/2657303970.py:18\n",
      "[2023-12-16 20:56:57,961] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,961] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                  ~~~~~~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,963] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST tensor_x [TensorVariable()]\n",
      "[2023-12-16 20:56:57,963] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:19\n",
      "[2023-12-16 20:56:57,963] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           output = self.fc3(tensor_x)\n",
      "[2023-12-16 20:56:57,963] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []\n",
      "[2023-12-16 20:56:57,963] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD fc3 [NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,964] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor_x [NullVariable, NNModuleVariable()]\n",
      "[2023-12-16 20:56:57,964] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,964] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, NNModuleVariable(), TensorVariable()]\n",
      "[2023-12-16 20:56:57,965] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call l__self___fc3 from forward /tmp/ipykernel_5842/2657303970.py:19\n",
      "[2023-12-16 20:56:57,965] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]       output = self.fc3(tensor_x)\n",
      "[2023-12-16 20:56:57,965] [2/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                ~~~~~~~~^^^^^^^^^^\n",
      "[2023-12-16 20:56:57,969] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST output [TensorVariable()]\n",
      "[2023-12-16 20:56:57,969] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line forward /tmp/ipykernel_5842/2657303970.py:20\n",
      "[2023-12-16 20:56:57,969] [2/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]           return output\n",
      "[2023-12-16 20:56:57,969] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST output []\n",
      "[2023-12-16 20:56:57,970] [2/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]\n",
      "[2023-12-16 20:56:57,970] [2/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-12-16 20:56:57,970] [2/0] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile\n",
      "[2023-12-16 20:56:57,970] [2/0] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_5842/2657303970.py, line 20 in forward>], graph_break=False)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_1 =====\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  <eval_with_key>.38 class GraphModule(torch.nn.Module):\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_tensor_x_ : torch.Tensor):\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_tensor_x_ = L_tensor_x_\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:13, code: tensor_x = self.fc0(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___fc0 = self.L__self___fc0(l_tensor_x_);  l_tensor_x_ = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:14, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sigmoid = torch.sigmoid(l__self___fc0);  l__self___fc0 = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:15, code: tensor_x = self.fc1(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___fc1 = self.L__self___fc1(sigmoid);  sigmoid = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:16, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sigmoid_1 = torch.sigmoid(l__self___fc1);  l__self___fc1 = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:17, code: tensor_x = self.fc2(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___fc2 = self.L__self___fc2(sigmoid_1);  sigmoid_1 = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:18, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         sigmoid_2 = torch.sigmoid(l__self___fc2);  l__self___fc2 = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:19, code: output = self.fc3(tensor_x)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___fc3 = self.L__self___fc3(sigmoid_2);  sigmoid_2 = None\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (l__self___fc3,)\n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         \n",
      "[2023-12-16 20:56:57,971] [2/0] torch._dynamo.output_graph.__graph_code: [DEBUG] \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG] Tabulate module missing, please install tabulate to log the graph in tabular format, logging code instead:\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]  ===== __compiled_fn_1 =====\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]  <eval_with_key>.38 class GraphModule(torch.nn.Module):\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]     def forward(self, L_tensor_x_ : torch.Tensor):\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         l_tensor_x_ = L_tensor_x_\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:13, code: tensor_x = self.fc0(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         l__self___fc0 = self.L__self___fc0(l_tensor_x_);  l_tensor_x_ = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:14, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         sigmoid = torch.sigmoid(l__self___fc0);  l__self___fc0 = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:15, code: tensor_x = self.fc1(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         l__self___fc1 = self.L__self___fc1(sigmoid);  sigmoid = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:16, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         sigmoid_1 = torch.sigmoid(l__self___fc1);  l__self___fc1 = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:17, code: tensor_x = self.fc2(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         l__self___fc2 = self.L__self___fc2(sigmoid_1);  sigmoid_1 = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:18, code: tensor_x = torch.sigmoid(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         sigmoid_2 = torch.sigmoid(l__self___fc2);  l__self___fc2 = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         # File: /tmp/ipykernel_5842/2657303970.py:19, code: output = self.fc3(tensor_x)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         l__self___fc3 = self.L__self___fc3(sigmoid_2);  sigmoid_2 = None\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         return (l__self___fc3,)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG]         \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph: [DEBUG] \n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] TRACED GRAPH TENSOR SIZES\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] ===== __compiled_fn_1 =====\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_tensor_x_: (97, 8)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___fc0: (97, 8)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] sigmoid: (97, 8)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___fc1: (97, 4)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] sigmoid_1: (97, 4)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___fc2: (97, 2)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] sigmoid_2: (97, 2)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___fc3: (97, 2)\n",
      "[2023-12-16 20:56:57,972] [2/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] \n",
      "[2023-12-16 20:56:57,973] [2/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function dynamo_normalization_capturing_compiler\n",
      "[2023-12-16 20:56:57,973] [2/0] torch._dynamo.output_graph: [INFO] Step 2: done compiler function dynamo_normalization_capturing_compiler\n",
      "[2023-12-16 20:56:57,975] [2/0] torch._dynamo.guards.__guards: [DEBUG] GUARDS:\n",
      "[2023-12-16 20:56:57,975] [2/0] torch._dynamo.guards.__guards: [DEBUG] ___check_obj_id(L['self'], 140192646151376)                   # tensor_x = self.fc0(tensor_x)  # mp/ipykernel_5842/2657303970.py:13 in forward\n",
      "[2023-12-16 20:56:57,976] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['self'].training == True                                    # tensor_x = self.fc0(tensor_x)  # mp/ipykernel_5842/2657303970.py:13 in forward\n",
      "[2023-12-16 20:56:57,976] [2/0] torch._dynamo.guards.__guards: [DEBUG] ___check_type_id(L['tensor_x'], 94043413262656)               # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,976] [2/0] torch._dynamo.guards.__guards: [DEBUG] str(L['tensor_x'].dtype) == 'torch.float32'                   # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,977] [2/0] torch._dynamo.guards.__guards: [DEBUG] str(L['tensor_x'].device) == 'cpu'                            # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,977] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].requires_grad == False                          # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,978] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].ndimension() == 2                               # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,978] [2/0] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['tensor_x'], '_dynamo_dynamic_indices') == False    # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,979] [2/0] torch._dynamo.guards.__guards: [DEBUG] str(L['tensor_x'].dtype) == 'torch.float32'                   # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,979] [2/0] torch._dynamo.guards.__guards: [DEBUG] str(L['tensor_x'].device) == 'cpu'                            # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,979] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].requires_grad == False                          # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,980] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].ndimension() == 2                               # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,980] [2/0] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['tensor_x'], '_dynamo_dynamic_indices') == False    # _dynamo/variables/builder.py:1248 in wrap_fx_proxy_cls\n",
      "[2023-12-16 20:56:57,980] [2/0] torch._dynamo.guards.__guards: [DEBUG] ___is_grad_enabled()                                          # _dynamo/output_graph.py:345 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,981] [2/0] torch._dynamo.guards.__guards: [DEBUG] not ___are_deterministic_algorithms_enabled()                 # _dynamo/output_graph.py:341 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,981] [2/0] torch._dynamo.guards.__guards: [DEBUG] ___is_torch_function_enabled()                                # _dynamo/output_graph.py:349 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,981] [2/0] torch._dynamo.guards.__guards: [DEBUG] utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:347 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,982] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].size()[0] == 97                                 # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,982] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].size()[1] == 8                                  # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,982] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].stride()[0] == 8                                # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,983] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].stride()[1] == 1                                # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,983] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].storage_offset() == 0                           # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,983] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].size()[1] == 8                                  # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,984] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].stride()[0] == 8                                # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,984] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].stride()[1] == 1                                # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,984] [2/0] torch._dynamo.guards.__guards: [DEBUG] L['tensor_x'].storage_offset() == 0                           # _dynamo/output_graph.py:339 in init_ambient_guards\n",
      "[2023-12-16 20:56:57,997] torch._dynamo.convert_frame: [DEBUG] skipping because no torch.* __init__             <string> 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.fc0 = nn.Linear(8, 8, bias=True)\n",
    "      self.fc1 = nn.Linear(8, 4, bias=True)\n",
    "      self.fc2 = nn.Linear(4, 2, bias=True)\n",
    "      self.fc3 = nn.Linear(2, 2, bias=True)\n",
    "\n",
    "  def forward(self, tensor_x: torch.Tensor):\n",
    "      tensor_x = self.fc0(tensor_x)\n",
    "      tensor_x = torch.sigmoid(tensor_x)\n",
    "      tensor_x = self.fc1(tensor_x)\n",
    "      tensor_x = torch.sigmoid(tensor_x)\n",
    "      tensor_x = self.fc2(tensor_x)\n",
    "      tensor_x = torch.sigmoid(tensor_x)\n",
    "      output = self.fc3(tensor_x)\n",
    "      return output\n",
    "\n",
    "model2 = MLPModel()\n",
    "tensor_x = torch.rand((97, 8), dtype=torch.float32)\n",
    "export_output = torch.onnx.dynamo_export(model2, tensor_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input[INT64, batch_sizex128]\n",
      ") initializers (\n",
      "  %token_emb.weight[FLOAT, 50257x128]\n",
      "  %pos_emb.weight[FLOAT, 128x128]\n",
      "  %blocks.0.mhsa.0.weight[FLOAT, 128]\n",
      "  %blocks.0.ffwd.0.weight[FLOAT, 128]\n",
      "  %blocks.1.mhsa.0.weight[FLOAT, 128]\n",
      "  %blocks.1.ffwd.0.weight[FLOAT, 128]\n",
      "  %rmsn.weight[FLOAT, 128]\n",
      "  %lm_head.bias[FLOAT, 50257]\n",
      "  %onnx::MatMul_325[FLOAT, 128x384]\n",
      "  %onnx::MatMul_329[FLOAT, 128x128]\n",
      "  %onnx::MatMul_330[FLOAT, 128x512]\n",
      "  %onnx::MatMul_331[FLOAT, 128x512]\n",
      "  %onnx::MatMul_332[FLOAT, 512x128]\n",
      "  %onnx::MatMul_333[FLOAT, 128x384]\n",
      "  %onnx::MatMul_337[FLOAT, 128x128]\n",
      "  %onnx::MatMul_338[FLOAT, 128x512]\n",
      "  %onnx::MatMul_339[FLOAT, 128x512]\n",
      "  %onnx::MatMul_340[FLOAT, 512x128]\n",
      "  %onnx::MatMul_341[FLOAT, 128x50257]\n",
      ") {\n",
      "  %/Shape_output_0 = Shape(%input)\n",
      "  %/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/Gather_output_0 = Gather[axis = 0](%/Shape_output_0, %/Constant_output_0)\n",
      "  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/Cast_output_0 = Cast[to = 7](%/Gather_output_0)\n",
      "  %/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/Range_output_0 = Range(%/Constant_1_output_0, %/Cast_output_0, %/Constant_2_output_0)\n",
      "  %/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/Unsqueeze_output_0 = Unsqueeze(%/Range_output_0, %/Constant_3_output_0)\n",
      "  %/token_emb/Gather_output_0 = Gather(%token_emb.weight, %input)\n",
      "  %/pos_emb/Gather_output_0 = Gather(%pos_emb.weight, %/Unsqueeze_output_0)\n",
      "  %/Add_output_0 = Add(%/token_emb/Gather_output_0, %/pos_emb/Gather_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Pow_output_0 = Pow(%/Add_output_0, %/blocks/blocks.0/mhsa/mhsa.0/Constant_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/blocks/blocks.0/mhsa/mhsa.0/Pow_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Add_output_0 = Add(%/blocks/blocks.0/mhsa/mhsa.0/ReduceMean_output_0, %/blocks/blocks.0/mhsa/mhsa.0/Constant_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Sqrt_output_0 = Sqrt(%/blocks/blocks.0/mhsa/mhsa.0/Add_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Div_output_0 = Div(%/blocks/blocks.0/mhsa/mhsa.0/Constant_2_output_0, %/blocks/blocks.0/mhsa/mhsa.0/Sqrt_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Mul_output_0 = Mul(%/Add_output_0, %/blocks/blocks.0/mhsa/mhsa.0/Div_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.0/Mul_1_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.0/Mul_output_0, %blocks.0.mhsa.0.weight)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Gather_output_0 = Gather[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Shape_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_1_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Gather_1_output_0 = Gather[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Shape_1_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_2_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Gather_2_output_0 = Gather[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Shape_2_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/qkv/MatMul_output_0 = MatMul(%/blocks/blocks.0/mhsa/mhsa.0/Mul_1_output_0, %onnx::MatMul_325)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_3_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.1/qkv/MatMul_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Gather_3_output_0 = Gather[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Shape_3_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_4_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Add_output_0 = Add(%/blocks/blocks.0/mhsa/mhsa.1/Gather_3_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_5_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Div_output_0 = Div(%/blocks/blocks.0/mhsa/mhsa.1/Add_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_6_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Mul_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_7_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_4_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Mul_1_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_8_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_1_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_1_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_9_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Mul_2_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_9_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_2_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_1_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_2_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_10_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Div_1_output_0 = Div(%/blocks/blocks.0/mhsa/mhsa.1/Gather_2_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_10_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Cast_output_0 = Cast[to = 7](%/blocks/blocks.0/mhsa/mhsa.1/Div_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Cast_1_output_0 = Cast[to = 7](%/blocks/blocks.0/mhsa/mhsa.1/Cast_output_0)\n",
      "  %onnx::Unsqueeze_75 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_75)\n",
      "  %onnx::Unsqueeze_77 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_1_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_77)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_11_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_81 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_2_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_81)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Concat_output_0 = Concat[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_1_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_11_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_2_output_0)\n",
      "  %onnx::Unsqueeze_84 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_3_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_84)\n",
      "  %onnx::Unsqueeze_86 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_4_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_86)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_90 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_5_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_90)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Concat_1_output_0 = Concat[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_3_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_4_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_12_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_5_output_0)\n",
      "  %onnx::Unsqueeze_93 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_6_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_93)\n",
      "  %onnx::Unsqueeze_95 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_7_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_95)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_99 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_8_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_99)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Concat_2_output_0 = Concat[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_6_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_7_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_13_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_8_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Reshape_output_0 = Reshape[allowzero = 0](%/blocks/blocks.0/mhsa/mhsa.1/Slice_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Concat_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.0/mhsa/mhsa.1/Reshape_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Reshape_1_output_0 = Reshape[allowzero = 0](%/blocks/blocks.0/mhsa/mhsa.1/Slice_1_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Concat_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.0/mhsa/mhsa.1/Reshape_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Reshape_2_output_0 = Reshape[allowzero = 0](%/blocks/blocks.0/mhsa/mhsa.1/Slice_2_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Concat_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.0/mhsa/mhsa.1/Reshape_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_4_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.1/Transpose_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_3_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/Shape_4_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_14_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_15_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Cast_2_output_0 = Cast[to = 1](%/blocks/blocks.0/mhsa/mhsa.1/Slice_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_output_0 = Sqrt(%/blocks/blocks.0/mhsa/mhsa.1/Cast_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_16_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Div_2_output_0 = Div(%/blocks/blocks.0/mhsa/mhsa.1/Constant_16_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_5_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.1/Transpose_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Shape_6_output_0 = Shape(%/blocks/blocks.0/mhsa/mhsa.1/Transpose_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_18_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_4_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/Shape_5_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_18_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_17_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Slice_5_output_0 = Slice(%/blocks/blocks.0/mhsa/mhsa.1/Shape_6_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_18_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_17_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Concat_3_output_0 = Concat[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Slice_4_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Slice_5_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Expand_output_0 = Expand(%/blocks/blocks.0/mhsa/mhsa.1/Constant_19_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Concat_3_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Trilu_output_0 = Trilu[upper = 0](%/blocks/blocks.0/mhsa/mhsa.1/Expand_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Equal_output_0 = Equal(%/blocks/blocks.0/mhsa/mhsa.1/Trilu_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_22_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Where_output_0 = Where(%/blocks/blocks.0/mhsa/mhsa.1/Equal_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_21_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Constant_20_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Transpose_3_output_0 = Transpose[perm = [0, 2, 3, 1]](%/blocks/blocks.0/mhsa/mhsa.1/Reshape_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_1_output_0 = Sqrt(%/blocks/blocks.0/mhsa/mhsa.1/Div_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Mul_3_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.1/Transpose_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_2_output_0 = Sqrt(%/blocks/blocks.0/mhsa/mhsa.1/Div_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Mul_4_output_0 = Mul(%/blocks/blocks.0/mhsa/mhsa.1/Transpose_3_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Sqrt_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/MatMul_output_0 = MatMul(%/blocks/blocks.0/mhsa/mhsa.1/Mul_3_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Mul_4_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Add_1_output_0 = Add(%/blocks/blocks.0/mhsa/mhsa.1/MatMul_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Where_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Softmax_output_0 = Softmax[axis = -1](%/blocks/blocks.0/mhsa/mhsa.1/Add_1_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/MatMul_1_output_0 = MatMul(%/blocks/blocks.0/mhsa/mhsa.1/Softmax_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Transpose_2_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.0/mhsa/mhsa.1/MatMul_1_output_0)\n",
      "  %onnx::Unsqueeze_141 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_9_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_141)\n",
      "  %onnx::Unsqueeze_143 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_10_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_143)\n",
      "  %onnx::Unsqueeze_145 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_11_output_0 = Unsqueeze(%/blocks/blocks.0/mhsa/mhsa.1/Gather_2_output_0, %onnx::Unsqueeze_145)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Concat_4_output_0 = Concat[axis = 0](%/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_9_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_10_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Unsqueeze_11_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/Reshape_3_output_0 = Reshape[allowzero = 0](%/blocks/blocks.0/mhsa/mhsa.1/Transpose_4_output_0, %/blocks/blocks.0/mhsa/mhsa.1/Concat_4_output_0)\n",
      "  %/blocks/blocks.0/mhsa/mhsa.1/proj/MatMul_output_0 = MatMul(%/blocks/blocks.0/mhsa/mhsa.1/Reshape_3_output_0, %onnx::MatMul_329)\n",
      "  %/blocks/blocks.0/Add_output_0 = Add(%/Add_output_0, %/blocks/blocks.0/mhsa/mhsa.1/proj/MatMul_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Pow_output_0 = Pow(%/blocks/blocks.0/Add_output_0, %/blocks/blocks.0/ffwd/ffwd.0/Constant_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/blocks/blocks.0/ffwd/ffwd.0/Pow_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Add_output_0 = Add(%/blocks/blocks.0/ffwd/ffwd.0/ReduceMean_output_0, %/blocks/blocks.0/ffwd/ffwd.0/Constant_1_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Sqrt_output_0 = Sqrt(%/blocks/blocks.0/ffwd/ffwd.0/Add_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Div_output_0 = Div(%/blocks/blocks.0/ffwd/ffwd.0/Constant_2_output_0, %/blocks/blocks.0/ffwd/ffwd.0/Sqrt_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Mul_output_0 = Mul(%/blocks/blocks.0/Add_output_0, %/blocks/blocks.0/ffwd/ffwd.0/Div_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.0/Mul_1_output_0 = Mul(%/blocks/blocks.0/ffwd/ffwd.0/Mul_output_0, %blocks.0.ffwd.0.weight)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/l1/MatMul_output_0 = MatMul(%/blocks/blocks.0/ffwd/ffwd.0/Mul_1_output_0, %onnx::MatMul_330)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/Sigmoid_output_0 = Sigmoid(%/blocks/blocks.0/ffwd/ffwd.1/l1/MatMul_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/Mul_output_0 = Mul(%/blocks/blocks.0/ffwd/ffwd.1/l1/MatMul_output_0, %/blocks/blocks.0/ffwd/ffwd.1/Sigmoid_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/l3/MatMul_output_0 = MatMul(%/blocks/blocks.0/ffwd/ffwd.0/Mul_1_output_0, %onnx::MatMul_331)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/Mul_1_output_0 = Mul(%/blocks/blocks.0/ffwd/ffwd.1/Mul_output_0, %/blocks/blocks.0/ffwd/ffwd.1/l3/MatMul_output_0)\n",
      "  %/blocks/blocks.0/ffwd/ffwd.1/l2/MatMul_output_0 = MatMul(%/blocks/blocks.0/ffwd/ffwd.1/Mul_1_output_0, %onnx::MatMul_332)\n",
      "  %/blocks/blocks.0/Add_1_output_0 = Add(%/blocks/blocks.0/Add_output_0, %/blocks/blocks.0/ffwd/ffwd.1/l2/MatMul_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Pow_output_0 = Pow(%/blocks/blocks.0/Add_1_output_0, %/blocks/blocks.1/mhsa/mhsa.0/Constant_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/blocks/blocks.1/mhsa/mhsa.0/Pow_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Add_output_0 = Add(%/blocks/blocks.1/mhsa/mhsa.0/ReduceMean_output_0, %/blocks/blocks.1/mhsa/mhsa.0/Constant_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Sqrt_output_0 = Sqrt(%/blocks/blocks.1/mhsa/mhsa.0/Add_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Div_output_0 = Div(%/blocks/blocks.1/mhsa/mhsa.0/Constant_2_output_0, %/blocks/blocks.1/mhsa/mhsa.0/Sqrt_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Mul_output_0 = Mul(%/blocks/blocks.0/Add_1_output_0, %/blocks/blocks.1/mhsa/mhsa.0/Div_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.0/Mul_1_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.0/Mul_output_0, %blocks.1.mhsa.0.weight)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Gather_output_0 = Gather[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Shape_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_1_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Gather_1_output_0 = Gather[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Shape_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_2_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.0/Mul_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Gather_2_output_0 = Gather[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Shape_2_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/qkv/MatMul_output_0 = MatMul(%/blocks/blocks.1/mhsa/mhsa.0/Mul_1_output_0, %onnx::MatMul_333)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_3_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.1/qkv/MatMul_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_3_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Gather_3_output_0 = Gather[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Shape_3_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_4_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_5_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Add_output_0 = Add(%/blocks/blocks.1/mhsa/mhsa.1/Gather_3_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_5_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_6_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Div_output_0 = Div(%/blocks/blocks.1/mhsa/mhsa.1/Add_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_6_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_7_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Mul_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_7_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_4_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_8_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Mul_1_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_8_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_1_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_9_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Mul_2_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.1/Div_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_9_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_2_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/qkv/MatMul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_2_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_10_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Div_1_output_0 = Div(%/blocks/blocks.1/mhsa/mhsa.1/Gather_2_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_10_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Cast_output_0 = Cast[to = 7](%/blocks/blocks.1/mhsa/mhsa.1/Div_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Cast_1_output_0 = Cast[to = 7](%/blocks/blocks.1/mhsa/mhsa.1/Cast_output_0)\n",
      "  %onnx::Unsqueeze_214 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_214)\n",
      "  %onnx::Unsqueeze_216 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_1_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_216)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_11_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_220 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_2_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_220)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Concat_output_0 = Concat[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_11_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_2_output_0)\n",
      "  %onnx::Unsqueeze_223 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_3_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_223)\n",
      "  %onnx::Unsqueeze_225 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_4_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_225)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_12_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_229 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_5_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_229)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Concat_1_output_0 = Concat[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_3_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_4_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_12_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_5_output_0)\n",
      "  %onnx::Unsqueeze_232 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_6_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_232)\n",
      "  %onnx::Unsqueeze_234 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_7_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_234)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_13_output_0 = Constant[value = <Tensor>]()\n",
      "  %onnx::Unsqueeze_238 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_8_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Cast_1_output_0, %onnx::Unsqueeze_238)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Concat_2_output_0 = Concat[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_6_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_7_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_13_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_8_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Reshape_output_0 = Reshape[allowzero = 0](%/blocks/blocks.1/mhsa/mhsa.1/Slice_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Concat_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Transpose_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.1/mhsa/mhsa.1/Reshape_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Reshape_1_output_0 = Reshape[allowzero = 0](%/blocks/blocks.1/mhsa/mhsa.1/Slice_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Concat_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Transpose_1_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.1/mhsa/mhsa.1/Reshape_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Reshape_2_output_0 = Reshape[allowzero = 0](%/blocks/blocks.1/mhsa/mhsa.1/Slice_2_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Concat_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Transpose_2_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.1/mhsa/mhsa.1/Reshape_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_4_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.1/Transpose_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_14_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_15_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_3_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/Shape_4_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_14_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_15_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Cast_2_output_0 = Cast[to = 1](%/blocks/blocks.1/mhsa/mhsa.1/Slice_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_output_0 = Sqrt(%/blocks/blocks.1/mhsa/mhsa.1/Cast_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_16_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Div_2_output_0 = Div(%/blocks/blocks.1/mhsa/mhsa.1/Constant_16_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_5_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.1/Transpose_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Shape_6_output_0 = Shape(%/blocks/blocks.1/mhsa/mhsa.1/Transpose_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_17_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_18_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_4_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/Shape_5_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_18_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_17_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Slice_5_output_0 = Slice(%/blocks/blocks.1/mhsa/mhsa.1/Shape_6_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_18_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_17_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Concat_3_output_0 = Concat[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Slice_4_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Slice_5_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_19_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Expand_output_0 = Expand(%/blocks/blocks.1/mhsa/mhsa.1/Constant_19_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Concat_3_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Trilu_output_0 = Trilu[upper = 0](%/blocks/blocks.1/mhsa/mhsa.1/Expand_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_20_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_21_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Constant_22_output_0 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Equal_output_0 = Equal(%/blocks/blocks.1/mhsa/mhsa.1/Trilu_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_22_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Where_output_0 = Where(%/blocks/blocks.1/mhsa/mhsa.1/Equal_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_21_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Constant_20_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Transpose_3_output_0 = Transpose[perm = [0, 2, 3, 1]](%/blocks/blocks.1/mhsa/mhsa.1/Reshape_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_1_output_0 = Sqrt(%/blocks/blocks.1/mhsa/mhsa.1/Div_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Mul_3_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.1/Transpose_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_2_output_0 = Sqrt(%/blocks/blocks.1/mhsa/mhsa.1/Div_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Mul_4_output_0 = Mul(%/blocks/blocks.1/mhsa/mhsa.1/Transpose_3_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Sqrt_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/MatMul_output_0 = MatMul(%/blocks/blocks.1/mhsa/mhsa.1/Mul_3_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Mul_4_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Add_1_output_0 = Add(%/blocks/blocks.1/mhsa/mhsa.1/MatMul_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Where_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Softmax_output_0 = Softmax[axis = -1](%/blocks/blocks.1/mhsa/mhsa.1/Add_1_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/MatMul_1_output_0 = MatMul(%/blocks/blocks.1/mhsa/mhsa.1/Softmax_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Transpose_2_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Transpose_4_output_0 = Transpose[perm = [0, 2, 1, 3]](%/blocks/blocks.1/mhsa/mhsa.1/MatMul_1_output_0)\n",
      "  %onnx::Unsqueeze_280 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_9_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_output_0, %onnx::Unsqueeze_280)\n",
      "  %onnx::Unsqueeze_282 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_10_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_1_output_0, %onnx::Unsqueeze_282)\n",
      "  %onnx::Unsqueeze_284 = Constant[value = <Tensor>]()\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_11_output_0 = Unsqueeze(%/blocks/blocks.1/mhsa/mhsa.1/Gather_2_output_0, %onnx::Unsqueeze_284)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Concat_4_output_0 = Concat[axis = 0](%/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_9_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_10_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Unsqueeze_11_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/Reshape_3_output_0 = Reshape[allowzero = 0](%/blocks/blocks.1/mhsa/mhsa.1/Transpose_4_output_0, %/blocks/blocks.1/mhsa/mhsa.1/Concat_4_output_0)\n",
      "  %/blocks/blocks.1/mhsa/mhsa.1/proj/MatMul_output_0 = MatMul(%/blocks/blocks.1/mhsa/mhsa.1/Reshape_3_output_0, %onnx::MatMul_337)\n",
      "  %/blocks/blocks.1/Add_output_0 = Add(%/blocks/blocks.0/Add_1_output_0, %/blocks/blocks.1/mhsa/mhsa.1/proj/MatMul_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Pow_output_0 = Pow(%/blocks/blocks.1/Add_output_0, %/blocks/blocks.1/ffwd/ffwd.0/Constant_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/blocks/blocks.1/ffwd/ffwd.0/Pow_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Add_output_0 = Add(%/blocks/blocks.1/ffwd/ffwd.0/ReduceMean_output_0, %/blocks/blocks.1/ffwd/ffwd.0/Constant_1_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Sqrt_output_0 = Sqrt(%/blocks/blocks.1/ffwd/ffwd.0/Add_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Div_output_0 = Div(%/blocks/blocks.1/ffwd/ffwd.0/Constant_2_output_0, %/blocks/blocks.1/ffwd/ffwd.0/Sqrt_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Mul_output_0 = Mul(%/blocks/blocks.1/Add_output_0, %/blocks/blocks.1/ffwd/ffwd.0/Div_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.0/Mul_1_output_0 = Mul(%/blocks/blocks.1/ffwd/ffwd.0/Mul_output_0, %blocks.1.ffwd.0.weight)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/l1/MatMul_output_0 = MatMul(%/blocks/blocks.1/ffwd/ffwd.0/Mul_1_output_0, %onnx::MatMul_338)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/Sigmoid_output_0 = Sigmoid(%/blocks/blocks.1/ffwd/ffwd.1/l1/MatMul_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/Mul_output_0 = Mul(%/blocks/blocks.1/ffwd/ffwd.1/l1/MatMul_output_0, %/blocks/blocks.1/ffwd/ffwd.1/Sigmoid_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/l3/MatMul_output_0 = MatMul(%/blocks/blocks.1/ffwd/ffwd.0/Mul_1_output_0, %onnx::MatMul_339)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/Mul_1_output_0 = Mul(%/blocks/blocks.1/ffwd/ffwd.1/Mul_output_0, %/blocks/blocks.1/ffwd/ffwd.1/l3/MatMul_output_0)\n",
      "  %/blocks/blocks.1/ffwd/ffwd.1/l2/MatMul_output_0 = MatMul(%/blocks/blocks.1/ffwd/ffwd.1/Mul_1_output_0, %onnx::MatMul_340)\n",
      "  %/blocks/blocks.1/Add_1_output_0 = Add(%/blocks/blocks.1/Add_output_0, %/blocks/blocks.1/ffwd/ffwd.1/l2/MatMul_output_0)\n",
      "  %/rmsn/Constant_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/rmsn/Pow_output_0 = Pow(%/blocks/blocks.1/Add_1_output_0, %/rmsn/Constant_output_0)\n",
      "  %/rmsn/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/rmsn/Pow_output_0)\n",
      "  %/rmsn/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/rmsn/Add_output_0 = Add(%/rmsn/ReduceMean_output_0, %/rmsn/Constant_1_output_0)\n",
      "  %/rmsn/Sqrt_output_0 = Sqrt(%/rmsn/Add_output_0)\n",
      "  %/rmsn/Constant_2_output_0 = Constant[value = <Scalar Tensor []>]()\n",
      "  %/rmsn/Div_output_0 = Div(%/rmsn/Constant_2_output_0, %/rmsn/Sqrt_output_0)\n",
      "  %/rmsn/Mul_output_0 = Mul(%/blocks/blocks.1/Add_1_output_0, %/rmsn/Div_output_0)\n",
      "  %/rmsn/Mul_1_output_0 = Mul(%/rmsn/Mul_output_0, %rmsn.weight)\n",
      "  %/lm_head/MatMul_output_0 = MatMul(%/rmsn/Mul_1_output_0, %onnx::MatMul_341)\n",
      "  %output = Add(%lm_head.bias, %/lm_head/MatMul_output_0)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(\"LMM.onnx\")\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’m not going to be able to get out of here.” said said, “but I’m not going to be able to get out of the.�\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "ort_session = ort.InferenceSession(\"harry_potter.onnx\")\n",
    "\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    #{\"input\": np.random.randint(0,len(vocab),(batch_size, context_length))},\n",
    "    {\"input\": np.array(vocab.encode(\"I’m not going to be able to get out of here,” he said, “but I’m not going to be able to get out of here.\"))[None,:]},\n",
    ")\n",
    "max = np.argmax(outputs,axis = -1)[0][0].tolist()\n",
    "print(vocab.decode(max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = HarryPotter('harry_potter.txt')\n",
    "val_set = HarryPotterDataset(context_length, books, vocab, train=False)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=False, pin_memory=True, num_workers=8)\n",
    "nll = model_c.evaluate_loss(val_loader, device)\n",
    "print(f\"---------------------\")\n",
    "print(f\"Validation NLL {f'{nll:.2f}':>6}\")\n",
    "print(f\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "with open(\"logs.txt\", \"a\") as file:\n",
    "        file.write(f\"model_file:{os.path.basename(inspect.getfile(LLM))}, eval_loss: {nll}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
